{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJBUQdyLihcy",
        "outputId": "0f5b73d7-86ef-434c-f1ba-5fe9c2e87f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph 0.5.0 requires pydantic>=2.7.4, but you have pydantic 2.6.4 which is incompatible.\n",
            "albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 2.6.4 which is incompatible.\n",
            "langchain 0.3.26 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 2.6.4 which is incompatible.\n",
            "langchain-core 0.3.66 requires pydantic>=2.7.4, but you have pydantic 2.6.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# # ATG Debate System (Colab Version)\n",
        "#\n",
        "# **Two-Agent Debate Simulation with LangGraph**\n",
        "#\n",
        "# This notebook implements:\n",
        "# - Scientist vs Philosopher debate\n",
        "# - 8 rounds of structured arguments\n",
        "# - Memory and state management\n",
        "# - Automated judging\n",
        "#\n",
        "# *Requires Anthropic API key for Claude models*\n",
        "\n",
        "# %% [code]\n",
        "# @title Step 1: Install Required Packages\n",
        "!pip install -q langgraph langchain python-dotenv\n",
        "!pip install -q pydantic==2.6.4  # Required for Colab compatibility\n",
        "\n",
        "# %% [code]\n",
        "# @title Step 2: Enter API Key\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "api_key = getpass(\"Enter your Anthropic API key: \")\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = api_key\n",
        "\n",
        "# %% [code]\n",
        "# @title Step 3: Define Core Components\n",
        "from langgraph.graph import Graph, END\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from typing import Dict, List, TypedDict\n",
        "import logging\n",
        "\n",
        "# Define state structure\n",
        "class DebateState(TypedDict):\n",
        "    topic: str\n",
        "    round: int\n",
        "    history: List\n",
        "    current_speaker: str\n",
        "    current_argument: str\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('debate_log.txt'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# %% [code]\n",
        "# @title Step 4: Create Debate Nodes\n",
        "# User Input Node\n",
        "def user_input_node(state: DebateState):\n",
        "    topic = input(\"Enter debate topic: \")\n",
        "    return {\"topic\": topic, \"round\": 0, \"history\": []}\n",
        "\n",
        "# Scientist Agent\n",
        "scientist_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a logical scientist. Present evidence-based arguments about risks and empirical data.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"Debate topic: {topic}\\nRound {round}: Present your scientific argument\"),\n",
        "])\n",
        "\n",
        "scientist_chain = scientist_prompt | ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
        "\n",
        "def scientist_node(state: DebateState):\n",
        "    response = scientist_chain.invoke({\n",
        "        \"topic\": state[\"topic\"],\n",
        "        \"round\": state[\"round\"],\n",
        "        \"history\": state[\"history\"]\n",
        "    })\n",
        "\n",
        "    logging.info(f\"[Round {state['round']}] Scientist: {response.content}\")\n",
        "    return {\n",
        "        \"history\": state[\"history\"] + [{\"role\": \"scientist\", \"content\": response.content}],\n",
        "        \"current_speaker\": \"scientist\",\n",
        "        \"current_argument\": response.content\n",
        "    }\n",
        "\n",
        "# Philosopher Agent\n",
        "philosopher_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a philosopher. Argue about ethics, morality, and societal implications.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"Debate topic: {topic}\\nRound {round}: Present your philosophical argument\"),\n",
        "])\n",
        "\n",
        "philosopher_chain = philosopher_prompt | ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
        "\n",
        "def philosopher_node(state: DebateState):\n",
        "    response = philosopher_chain.invoke({\n",
        "        \"topic\": state[\"topic\"],\n",
        "        \"round\": state[\"round\"],\n",
        "        \"history\": state[\"history\"]\n",
        "    })\n",
        "\n",
        "    logging.info(f\"[Round {state['round']}] Philosopher: {response.content}\")\n",
        "    return {\n",
        "        \"history\": state[\"history\"] + [{\"role\": \"philosopher\", \"content\": response.content}],\n",
        "        \"current_speaker\": \"philosopher\",\n",
        "        \"current_argument\": response.content\n",
        "    }\n",
        "\n",
        "# Judge Node\n",
        "judge_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert debate judge. Analyze this debate and:\n",
        "    1. Summarize key points from both sides\n",
        "    2. Evaluate argument strength\n",
        "    3. Declare a winner with specific reasons\"\"\"),\n",
        "    (\"human\", \"\"\"Debate Topic: {topic}\n",
        "\n",
        "    Full Debate History:\n",
        "    {history}\n",
        "\n",
        "    Please provide your verdict:\"\"\")\n",
        "])\n",
        "\n",
        "judge_chain = judge_prompt | ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
        "\n",
        "def judge_node(state: DebateState):\n",
        "    # Format history for judge\n",
        "    debate_transcript = \"\\n\".join(\n",
        "        f\"[{msg['role'].title()}] {msg['content']}\"\n",
        "        for msg in state[\"history\"]\n",
        "    )\n",
        "\n",
        "    response = judge_chain.invoke({\n",
        "        \"topic\": state[\"topic\"],\n",
        "        \"history\": debate_transcript\n",
        "    })\n",
        "\n",
        "    logging.info(f\"\\n[JUDGE VERDICT]\\n{response.content}\")\n",
        "    return {\"verdict\": response.content}\n",
        "\n",
        "# %% [code]\n",
        "# @title Step 5: Build the Workflow\n",
        "workflow = Graph()\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"user_input\", user_input_node)\n",
        "workflow.add_node(\"scientist\", scientist_node)\n",
        "workflow.add_node(\"philosopher\", philosopher_node)\n",
        "workflow.add_node(\"judge\", judge_node)\n",
        "\n",
        "# Set up edges\n",
        "workflow.set_entry_point(\"user_input\")\n",
        "\n",
        "# Define routing logic\n",
        "def route_rounds(state: DebateState):\n",
        "    if state[\"round\"] >= 8:\n",
        "        return \"judge\"\n",
        "    elif state[\"round\"] % 2 == 1:  # Odd rounds: scientist\n",
        "        return \"scientist\"\n",
        "    else:  # Even rounds: philosopher\n",
        "        return \"philosopher\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"user_input\",\n",
        "    lambda x: \"scientist\",  # First round always scientist\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"scientist\",\n",
        "    route_rounds\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"philosopher\",\n",
        "    route_rounds\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"judge\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "# %% [code]\n",
        "# @title Step 6: Run the Debate\n",
        "print(\"\\n=== ATG Debate System ===\")\n",
        "print(\"Starting debate...\\n\")\n",
        "\n",
        "# Initialize state\n",
        "state = {\"round\": 0}\n",
        "\n",
        "# Get user input\n",
        "state = user_input_node(state)\n",
        "\n",
        "# Run 8 rounds alternating between agents\n",
        "for round_num in range(1, 9):\n",
        "    state[\"round\"] = round_num\n",
        "\n",
        "    if round_num % 2 == 1:\n",
        "        state = scientist_node(state)\n",
        "    else:\n",
        "        state = philosopher_node(state)\n",
        "\n",
        "# Final judgment\n",
        "verdict = judge_node(state)\n",
        "\n",
        "print(\"\\n=== Debate Concluded ===\")\n",
        "print(verdict[\"verdict\"])\n",
        "\n",
        "# %% [code]\n",
        "# @title Step 7: View Logs\n",
        "print(\"\\nLog file contents:\")\n",
        "!cat debate_log.txt\n",
        "\n",
        "# %% [markdown]\n",
        "# **How to Use:**\n",
        "# 1. Run all cells (Runtime > Run all)\n",
        "# 2. When prompted, enter your Anthropic API key\n",
        "# 3. Enter debate topic when asked\n",
        "# 4. Watch the debate unfold in the output\n",
        "# 5. Final verdict will appear at the end\n",
        "#\n",
        "# **Notes:**\n",
        "# - Uses Claude Haiku for agents (fast/affordable)\n",
        "# - Uses Claude Opus for judging (higher quality)\n",
        "# - Full log saved to `debate_log.txt`\n",
        "# - Change models in the agent definitions if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRlD1HavikSf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}